{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 1234\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\logger_connector\\logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "Missing logger folder: E:\\M Tech\\6. Seminar\\SeminarCode\\nlp\\lightning_logs\n",
      "C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "\n",
      "  | Name       | Type              | Params\n",
      "-------------------------------------------------\n",
      "0 | bert_model | BertModel         | 109 M \n",
      "1 | criterion  | TripletMarginLoss | 0     \n",
      "-------------------------------------------------\n",
      "109 M     Trainable params\n",
      "0         Non-trainable params\n",
      "109 M     Total params\n",
      "437.929   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b11ac1f4c748f4aeaf2baf120a48f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# ---------------------\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import click\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AdamW\n",
    "\n",
    "class NL_Dataset(Dataset):\n",
    "\n",
    "\tdef __init__(self, ds_root, mode=\"train\"):\n",
    "\n",
    "\t\tassert mode in [\"train\", \"val\"]\n",
    "\t\tself.ds_root = ds_root\n",
    "\t\tself.mode = mode\n",
    "\n",
    "\t\t# load train/val keys\n",
    "\t\tmode_files = {\"train\": \"data/train.txt\", \"val\": \"data/validation.txt\"}\n",
    "\t\tsplit_file = os.path.join(\"E:/M Tech/6. Seminar/SeminarCode\", mode_files[mode])\n",
    "\n",
    "\t\t# read split uuids\n",
    "\t\twith open(split_file) as f:\n",
    "\t\t\tself.uuids = [line.rstrip() for line in f]\n",
    "\t\t\tf.close()\n",
    "\n",
    "\t\t# Load train split json\n",
    "\t\ttracks_root = os.path.join(ds_root, 'data/train-tracks.json')\n",
    "\t\twith open(tracks_root, \"r\") as f:\n",
    "\t\t\ttracks = json.load(f)\n",
    "\t\t\tf.close()\n",
    "\n",
    "\t\tself.tracks = tracks\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\t# type: () -> int\n",
    "\t\treturn len(self.uuids)\n",
    "\n",
    "\tdef __getitem__(self, item):\n",
    "\t\t# type: (int) -> (tuple, tuple)\n",
    "\n",
    "\t\tuuid = self.uuids[item]\n",
    "\t\tpos = self.tracks[uuid][\"nl\"]  # positive\n",
    "\t\tnp = torch.randperm(3)\n",
    "\n",
    "\t\tpositive = pos[np[0]]\n",
    "\t\tanchor = pos[np[1]]\n",
    "\n",
    "\t\t# pick uuid of negative embedding\n",
    "\t\tif self.mode != \"val\":\n",
    "\t\t\t# For negative samples pick a random one\n",
    "\t\t\tix = item\n",
    "\t\t\twhile ix == item:\n",
    "\t\t\t\t# Avoid picking the real index\n",
    "\t\t\t\tix = random.randint(0, len(self.uuids) - 1)\n",
    "\t\t\tnegative_uuid = self.uuids[ix]\n",
    "\t\telse:\n",
    "\t\t\t# Make validation deterministic\n",
    "\t\t\tnegative_uuid = self.uuids[(item ** 2 + 8) % len(self.uuids)]\n",
    "\n",
    "\t\t# negative\n",
    "\t\tneg = self.tracks[negative_uuid][\"nl\"]  # positive (3)\n",
    "\t\tnp = torch.randperm(3)\n",
    "\t\tnegative = neg[np[0]]\n",
    "\n",
    "\t\treturn anchor, positive, negative\n",
    "\n",
    "class LiteBert(pl.LightningModule):\n",
    "\tdef __init__(self, learning_rate=1e-4):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.save_hyperparameters()\n",
    "\t\tself.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\t\tself.bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\t\t# loss\n",
    "\t\tself.criterion = torch.nn.TripletMarginLoss(margin=2.5)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t# use forward for inference/predictions\n",
    "\t\ttokens = self.tokenizer(x, padding='longest', return_tensors='pt')\n",
    "\t\tmask = tokens['attention_mask'].to(self.device)\n",
    "\t\tbert_out = self.bert_model(tokens['input_ids'].to(self.device),\n",
    "\t\t\t\t\t\t\t\t   attention_mask=mask).last_hidden_state\n",
    "\n",
    "\t\tbert_out = torch.mean(bert_out, dim=1)\n",
    "\n",
    "\t\treturn bert_out  # (BS, 3, 768)\n",
    "\n",
    "\tdef training_step(self, batch, batch_idx):\n",
    "\t\tanchor, pos, neg = batch\n",
    "\t\tanchor_emb = self.forward(anchor)\n",
    "\t\tpos_emb = self.forward(pos)\n",
    "\t\tneg_emb = self.forward(neg)\n",
    "\n",
    "\t\tloss = self.criterion(anchor_emb, pos_emb, neg_emb)\n",
    "\t\tself.log('train_loss', loss, on_epoch=True)\n",
    "\t\treturn loss\n",
    "\n",
    "\tdef validation_step(self, batch, batch_idx):\n",
    "\t\tloss = self.training_step(batch, batch_idx)\n",
    "\t\tself.log('valid_loss', loss, on_epoch=True)\n",
    "\t\treturn loss\n",
    "\n",
    "\n",
    "\tdef configure_optimizers(self):\n",
    "\t\t# self.hparams available because we called self.save_hyperparameters()\n",
    "\t\treturn AdamW(self.parameters(), lr=self.hparams.learning_rate)\n",
    "\n",
    "def colate_fn(batch):\n",
    "\tpos = [s for b in batch for s in b[0]]\n",
    "\tneg = [s for b in batch for s in b[1]]\n",
    "\n",
    "\treturn pos, neg\n",
    "\n",
    "# @click.command()\n",
    "# @click.option('--ds_root', type=click.Path(exists=True),\n",
    "# \t\t\t  default='E:/M Tech/6. Seminar/SeminarCode')\n",
    "def main(ds_root=\"E:/M Tech/6. Seminar/SeminarCode\"):\n",
    "\tpl.seed_everything(1234)\n",
    "\n",
    "\ttrain = NL_Dataset(ds_root, mode=\"train\")\n",
    "\tval = NL_Dataset(ds_root, mode=\"val\")\n",
    "\n",
    "\t# dataloader\n",
    "\ttrain_loader = DataLoader(train, batch_size=48, num_workers=8, shuffle=True)\n",
    "\tval_loader = DataLoader(val, batch_size=8, num_workers=4, shuffle=False)\n",
    "\n",
    "\t# model\n",
    "\tmodel = LiteBert(learning_rate=1e-4)\n",
    "\n",
    "\t# train\n",
    "\ttrainer = pl.Trainer(max_epochs=4, gpus=1, accelerator='cpu', accumulate_grad_batches=4)\n",
    "\ttrainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "\tif trainer.global_rank == 0:\n",
    "\t\t# Save pretrained dict\n",
    "\t\tmodel.bert_model.save_pretrained('bert_ft_experimental')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\tmain()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
